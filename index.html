
<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yufan Ren</title>
  
  <meta name="author" content="Yufan Ren">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yufan Ren</name>
              </p>
              <p>Hello! I am currently a direct Ph.D. student (2020-) at <a href="https://www.epfl.ch/labs/ivrl/">Image and Visual Representation Lab</a>, part of the <a href="https://www.epfl.ch/schools/ic/">School of Computer and Communication Sciences</a> at <a href="epfl.ch">EPFL, Switzerland</a>. Under the guidance of Prof. <a href="https://people.epfl.ch/sabine.susstrunk">Sabine S√ºsstrunk</a> and Dr. <a href="https://sites.google.com/view/tong-zhang?pli=1">Tong Zhang</a>, I work on 3D Reconstruction and Perception, Image Diffusion, and Large Vision-Language Models (LVLMs). 
		</p>
		<p>
	 	During my Ph.D., I did two internships, one <a href="https://www.nvidia.com/en-us/">NVIDIA Zurich</a> on learning-based robot perception under <a href=https://alexmillane.github.io/>Dr. Alexander Millane</a>, another at <a href=https://research.facebook.com/>Meta London</a> under <a href=https://www.fkokkinos.com/>Dr. Filippos Kokkinos</a>.   
              </p>
              <p>
                Before my Ph.D. study, I earned my bachelor's degree from <a href="https://www.zju.edu.cn/english/">Zhejiang University, Hangzhou</a>, where I was honored to receive the Chu Kochen Award. 
              </p>
		<p style="color: red;">
		I am actively seeking opportunities for industry jobs and postdoctoral positions in 2025.
		</p>
      <img src="images/meta.png"  height="24">      
      <img src="images/nvidia.png"  height="26">      
      <img src="images/epfl-logo.png"  height="26">
      <img src="images/Zhejiang_University_Logotype1.svg.png"  height="27">
              <p style="text-align:center">
                <a href="mailto:yufan.ren@epfl.ch">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=dBc-t1cAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yufan-ren/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/ryf1123/">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.png" class="hoverZoomLink"></a>
            </td>
          </tr>
            
        </tbody></table>        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              &#127881 [Feb 2025] Our paper on Text-guided Image Editing is accepted by CVPR 2025!
            </p>
            <p>
              &#127881 [Jan 2025] Our paper on low-overlapping Point Cloud Registration (PCR) is accepted by TMLR 2025!
            </p>
	    <p>
              &#127881 [Aug 2024] I'm glad to share that I will work as an Research Scientist Intern on the GenAI LlaMA team at Meta London!
            <p>
	    <p>
              &#127881 [March 2024] I'm glad to share that starting this March I will work as an Intern on 3D perception at NVIDIA in Zurich with the <a href="https://nvidia-isaac-ros.github.io/concepts/scene_reconstruction/nvblox/index.html">nvblox team</a>!
            <p>
              &#127881 [Oct, 2023]: The next <a href="https://iccp-conference.org/iccp2024/"><b>International Conference on Computational Photography (ICCP)</b></a> is happening at EPFL, Switzerland in 2024. I am excited to announce my role as website chair.
            </p>
            <p>
              &#127881 [Sept 2023]: I am invited for a talk at <a href="https://lu.ma/lauzhack-aws-cloud">AWS LauzHack Cloud Research Day</a> on DeepFakes detection.
            </p>
            <p>
              &#127881 [Aprl 2023]: I am admitted to the <a href="https://iplab.dmi.unict.it/icvss2023/Home">International Computer Vision Summer School (ICVSS)</a>. See you in Sicily &#127965!
            </p>
            <p>
              &#127881 [Aprl 2023]: My paper on generalizable implicit reconstruction VolRecon is accepted by CVPR!
            </p>
          </td>


        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Research</heading>
          </td>


        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
        <tr onmouseout="fds_stop()" onmouseover="fds_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='fds_image'>
                <img src='images/coffee2.png' width="160"  height="140"></div>
              <img src='images/coffee1.png' width="160"  height="140">
            </div>
            <script type="text/javascript">
              function fds_start() {
                document.getElementById('fds_image').style.opacity = "1";
              }

              function fds_stop() {
                document.getElementById('fds_image').style.opacity = "0";
              }
              fds_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://ivrl.github.io/fds-webpage/">
              <papertitle>Text-Guided Latent Diffusion Image Editing</papertitle>
            </a>
            <br>
            <strong>Yufan Ren</strong>,
            <a href="https://zicongjiang.github.io/zicong-jiang/">Zicong Jiang</a>,
            <a href="https://sites.google.com/view/tong-zhang">Tong Zhang</a>,
            <a href="https://orbit.dtu.dk/en/persons/s%C3%B8ren-otto-forchhammer">S√∏ren Otto Forchhammer</a>,
            <a href="https://people.epfl.ch/sabine.susstrunk">Sabine
              S√ºsstrunk</a>
            <br>
            <em>CVPR'25</em>, <a href="https://ivrl.github.io/fds-webpage/">Project Page</a>
            <br>
            <p>
              In this paper, we analyze these failure cases of Text-guided image editing and introduce a simple yet effective approach that enables selective optimization of specific frequency bands within spatially localized regions.
            </p>
          </td>
        </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
        <tr onmouseout="diffpcr_stop()" onmouseover="diffpcr_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='diffpcr_image'>
                <img src='images/diffusionpcr-img2.png' width="160"  height="140"></div>
              <img src='images/diffusionpcr-img1.png' width="160"  height="140">
            </div>
            <script type="text/javascript">
              function diffpcr_start() {
                document.getElementById('diffpcr_image').style.opacity = "1";
              }

              function diffpcr_stop() {
                document.getElementById('diffpcr_image').style.opacity = "0";
              }
              diffpcr_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="./subpage/adaptive-pcr/index.html">
              <papertitle>Adaptive Multi-step Refinement Network for Robust Point Cloud Registration</papertitle>
            </a>
            <br>

            <a href="https://scholar.google.com/citations?user=QKCQWjcAAAAJ">Zhi Chen*</a>,
            <strong>Yufan Ren*</strong>,
            <a href="https://sites.google.com/view/tong-zhang">Tong Zhang</a>,
            <a href="https://scholar.google.ch/citations?user=o8BdwuMAAAAJ&hl=en">Zheng Dang</a>,
            <a href="https://scholar.google.com/citations?user=jRDPE2AAAAAJ&hl=en">Wenbing Tao</a>,
            <a href="https://people.epfl.ch/sabine.susstrunk">Sabine
              S√ºsstrunk</a>,
            <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a>
          
            <br>
            <em>TMLR'25</em>, <a href="./subpage/adaptive-pcr/index.html">Project Page</a>
            <br>
            <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a>
            / -->
            <!-- <a href="https://arxiv.org/abs/2201.00392">arXiv</a> -->
            <p></p>
            <p>
              Point Cloud Registration (PCR) estimates the relative rigid transformation between two point clouds. We propose an adaptive multi-step refinement network that refines the registration quality at each step by leveraging the information from the preceding step, achieving state-of-the-art performance on both the 3DMatch/3DLoMatch and KITTI benchmarks. Notably, on 3DLoMatch, our method reaches 80.4% recall rate, with an absolute improvement of 1.2%. 
            </p>
          </td>
        </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr onmouseout="volrecon_stop()" onmouseover="volrecon_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='volrecon_image'>
                  <img src='images/volrecon_after.jpeg' width="160"  height="140"></div>
                <img src='images/volrecon_before.jpeg' width="160"  height="140">
              </div>
              <script type="text/javascript">
                function volrecon_start() {
                  document.getElementById('volrecon_image').style.opacity = "1";
                }

                function volrecon_stop() {
                  document.getElementById('volrecon_image').style.opacity = "0";
                }
                volrecon_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="./subpage/VolRecon/index.html">
                <papertitle>VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction</papertitle>
              </a>
              <br>
              <strong>Yufan Ren*</strong>,
              <a href="https://fangjinhuawang.github.io//">Fangjinhua Wang*</a>,
              <a href="https://sites.google.com/view/tong-zhang">Tong Zhang</a>, 
							<a href="http://cvg.ethz.ch/people/faculty/">Marc Pollefeys</a>,
              <a href="https://people.epfl.ch/sabine.susstrunk">Sabine S√ºsstrunk</a>
              <br>
              <em>CVPR'23</em>, <a href="http://arxiv.org/abs/2212.08067">arXiv</a>, <a href="https://fangjinhuawang.github.io/VolRecon/">Project Page</a>, <a href="https://github.com/IVRL/VolRecon/">Code</a>
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a>
              / -->
              <!-- <a href="https://arxiv.org/abs/2201.00392">arXiv</a> -->
              <p></p>
              <p>
              We introduce VolRecon, a novel generalizable implicit reconstruction method with Signed Ray Distance Function (SRDF). To reconstruct the scene with fine details and little noise, VolRecon combines projection features aggregated from multi-view features, and volume features interpolated from a coarse global feature volume.
              </p>
            </td>
          </tr>

          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/v1-2.png' width="160" height="160"></div>
                <img src='images/v1.png' width="160" height="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20622">
                <papertitle>Learning V1 Simple Cells with Vector Representation of Local Content and Matrix Representation of Local Motion</papertitle>
              </a>
              <br>
              <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
              <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a>, 
							<a href="https://siyuanhuang.com/">Siyuan Huang</a>,
              <strong>Yufan Ren</strong>,
              <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>,
              <a href="https://scholar.google.com/citations?user=7k_1QFIAAAAJ&hl=en">Ying Nian Wu</a>
              <br>
              <em>AAAI'22</em>, <a href="https://arxiv.org/abs/1902.03871">arXiv</a>
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a>
              / -->
              <!-- <a href="https://arxiv.org/abs/2201.00392">arXiv</a> -->
              <p></p>
              <p>
                We propose a representational model for image pairs such as consecutive video frames that are related by local pixel displacements, in the hope that the model may shed light on motion perception in primary visual cortex (V1). The model couples the following two components: (1) the vector representations of local contents of images and (2) the matrix representations of local pixel displacements caused by the relative motions between the agent and the objects in the 3D scene.
              </p>
            </td>
          </tr>

          
          <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
          <div class="two" id='pnf_image'>
            <img src='images/bmvs2.png' width="160" height="140"></div>
          <img src='images/bmvs1.png' width="160" height="140">
          </div>
          <script type="text/javascript">
          function pnf_start() {
            document.getElementById('pnf_image').style.opacity = "1";
          }

          function pnf_stop() {
            document.getElementById('pnf_image').style.opacity = "0";
          }
          pnf_stop()
          </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Yao_BlendedMVS_A_Large-Scale_Dataset_for_Generalized_Multi-View_Stereo_Networks_CVPR_2020_paper.html">
          <papertitle>Blendedmvs: A large-scale dataset for generalized multi-view stereo networks</papertitle>
          </a> <br>
          <a href="https://yoyo000.github.io/">Yao Yao</a>,
          <a href="https://lzx551402.github.io/">Zixin Luo</a>,
          <a href="https://scholar.google.com.hk/citations?user=YR1MdT0AAAAJ&hl=zh-CN">Shiwei Li</a>,
          <a href="https://scholar.google.com/citations?user=diIjfmMAAAAJ&hl=en">Jingyang Zhang</a>,
          <strong>Yufan Ren</strong>,
          <a href="https://zlthinker.github.io/">Lei Zhou</a>,
          <a href="https://scholar.google.com/citations?user=CtpU8mUAAAAJ&hl=en">Tian Fang</a>,
          <a href="https://www.cse.ust.hk/~quan/">Long Quan</a>
          <br>
          <em>CVPR'20</em>, <a href="https://arxiv.org/abs/1911.10127">arXiv</a>, <a href="https://github.com/YoYo000/BlendedMVS">Dataset</a>
          <p>
            We introduce BlendedMVS, a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. To create the dataset, we apply a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, we render these mesh models to color images and depth maps. To introduce the ambient lighting information during training, the rendered color images are further blended with the input images to generate the training input.
          </p>
          </td>
          </tr> 
              </p>
            </td>
          </tr>
          
        </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Selected Projects and Activities</heading>
          <p>
            <!-- My current research interests mainly lie in the field of 3D vision, Neural Rendering, and AIGC. -->
          </p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr onmouseout="icvss_stop()" onmouseover="icvss_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='icvss_image'>
              <img src='images/sicily.png' width="160" height="140"></div>
            <img src='images/icvss-logo.png' width="160" height="140">
          </div>
          <script type="text/javascript">
            function icvss_start() {
              document.getElementById('icvss_image').style.opacity = "1";
            }

            function icvss_stop() {
              document.getElementById('icvss_image').style.opacity = "0";
            }
            icvss_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://iplab.dmi.unict.it/icvss2023/">
            <papertitle>International Computer Vision Summer School 2023</papertitle>
          </a>
          <br>
          <em>Universit√† di Catania, </em> 
          <br>
          <p></p>
          <p>
            The school aims to provide a stimulating opportunity for young researchers and Ph.D. students. The participants will benefit from direct interaction and discussions with world leaders in Computer Vision. Participants will also have the possibility to present the results of their research, and to interact with their scientific peers, in a friendly and constructive environment.
          </p>
        </td>
      </tr>

      <tr onmouseout="tmc_stop()" onmouseover="tmc_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='tmc_image'>
              <img src='images/tmc2.jpg' width="160" height="140"></div>
            <img src='images/tmc1.jpg' width="160" height="140">
          </div>
          <script type="text/javascript">
            function tmc_start() {
              document.getElementById('tmc_image').style.opacity = "1";
            }

            function tmc_stop() {
              document.getElementById('tmc_image').style.opacity = "0";
            }
            tmc_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://actu.epfl.ch/news/deepfake-generation-and-detection-is-like-an-arm-3/">
            <papertitle>Multimodal Fake Media Detection: AI Singapore Trusted Media Challenge</papertitle>
          </a>
          <br>
          <em>AI Singapore 2022</em>, <a href="https://actu.epfl.ch/news/deepfake-generation-and-detection-is-like-an-arm-3/">EPFL News</a> 
          <br>
          <p></p>
          <p>
          <a href="https://www.linkedin.com/in/petergro/?locale=en_US">Peter Gr√∂nquist</a> and I did <a hred="https://trustedmedia.aisingapore.org/">this challenge</a> and won the 100,000 USD prize (incl. grant). In this challenge, we design machine learning models to detect three types of fakeness, i.e., fake faces (DeepFakes), manipulated audio, and mis-synchronization (lip-sync), and use engineering tricks to make it fast. 
          </p>
        </td>
      </tr>

      
    </tbody>
    </table>
      <!-- <a href="https://clustrmaps.com/site/1bt5b"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=ThSjmYvIIyjXawrcSQ1RVYqBTQpwqPy6kXkk-ahKaN8&cl=ffffff" /></a> -->
      <div class="widgetContainer" style="width:200px; margin-left: 30px;">
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=ThSjmYvIIyjXawrcSQ1RVYqBTQpwqPy6kXkk-ahKaN8"></script>
      </div>
      <br>
	    Thanks for the <a href="https://github.com/jonbarron/website">awesome template</a> of Jon Barron. 
      </td>
    </tr>


    <!-- <tr>
      thanks for the awesome template
    </tr> -->
  </table>

  

</body>

</html>
